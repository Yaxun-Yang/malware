import pandas as pd
import keras.backend as K
import numpy as np
from keras.layers import Input, Dense
from keras.models import Model
from keras.optimizers import Adam
from keras.initializers import VarianceScaling
from keras.engine.topology import Layer, InputSpec
from keras import callbacks
from sklearn.cluster import KMeans
import metrics
from time import time


def read_data():
    temp_data = pd.read_json("base_data/opcode_point_sample.json", orient="index").to_numpy()[0]
    data = np.empty([13141, 256, 3])
    for i in range(len(temp_data)):
        data[i] = np.array(temp_data[i])

    return data


def auto_encoder(dims, init):
    # input
    x = Input(shape=dims[0], name="input")
    h = x

    # internal layers in encoder
    for i in dims[1:-1]:
        # print(i)
        h = Dense(i, activation='relu', kernel_initializer=init)(h)
    h = Dense(dims[-1], kernel_initializer=init, name="hidden")(h)

    # print("-------")
    # internal layer in decoder
    y = h
    for i in dims[-2:0:-1]:
        # print(i)
        y = Dense(i, kernel_initializer=init)(y)
    y = Dense(dims[0], kernel_initializer=init,  name="output")(y)
    return Model(inputs=x, outputs=y, name='AE'), Model(inputs=x, outputs=h, name='encoder')


class ClusteringLayer(Layer):
    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):
        super(ClusteringLayer, self).__init__(**kwargs)
        self.n_clusters = n_clusters
        self.alpha = alpha
        self.init_weights = weights
        self.input_spec = InputSpec(ndim=2)

    def build(self, input_shape):
        input_dim = input_shape[1]
        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))
        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform',
                                        name='clusters')
        if self.init_weights is not None:
            self.set_weights(self.init_weights)
            del self.init_weights
        self.built = True

    def call(self, inputs, **kwargs):
        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))
        q **= (self.alpha + 1.0) / 2.0
        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))
        return q

    def compute_output_shape(self, input_shape):
        assert input_shape and len(input_shape) == 2
        return input_shape[0], self.n_clusters

    def get_config(self):
        config = {"n_clusters": self.n_clusters}
        base_config = super(ClusteringLayer, self).get_config()
        return dict(list(base_config.items())+list(config.items()))


class DEC(object):
    def __init__(self, dims, n_cluster, init, alpha=1.0):
        super(DEC, self).__init__()

        self.dims = dims
        self.input_dim = dims[0]
        self.n_stacks = len(self.dims) - 1
        self.n_cluster = n_cluster
        self.alpha = alpha
        self.autoencoder, self.encoder = auto_encoder(dims, init)

        clustering_layer = ClusteringLayer(self.n_cluster, name='clustering')(self.encoder.output)

        self.model = Model(inputs=self.encoder.inputs, outputs=clustering_layer)

    def pretrain(self, x, optimizer, epochs, batch_size, save_dir='processed_data/temp'):
        print(".....Pretraining........")
        self.autoencoder.compile(optimizer=optimizer, loss='mse')

        csv_logger = callbacks.CSVLogger(save_dir + '/pretrain_log.csv')
        cb = [csv_logger]

        t0 = time()
        self.autoencoder.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=cb)
        print("Pretrain time: %ds" % round(time()-t0))
        self.autoencoder.save_weights(save_dir+'/ae_weights.h5')
        print('Pretrained weights are saved ')
        self.pretrained = True

    def load_weights(self, weights):
        self.model.load_weights(weights)

    def extract_features(self, x):
        return self.encoder.predict(x)

    def predict(self, x):
        q = self.model.predict(x, verbose=0)
        return q.argmax(1)

    @staticmethod
    def target_distribution(q):
        weight = q ** 2 / q.sum(0)
        return (weight.T / weight.sum(1)).T

    def compile(self, optimizer, loss="kld"):
        self.model.compile(optimizer=optimizer, loss=loss)

    def fit(self, x, maxiter, batch_size, tol=1e-3, update_interval=140, save_dir="processed_data/temp"):
        print('Update interval', update_interval)
        save_interval = int(x.shape[0] / batch_size) * 5
        print("Save interval", save_interval)

        t1 = time()
        print('Initializing cluster centers with k-means.')
        kmeans = KMeans(n_clusters=self.n_cluster, n_init=20)
        y_pred = kmeans.fit_predict(self.encoder.predict(x))
        y_pred_last = np.copy(y_pred)
        self.model.get_layer(name="clustering").set_weights([kmeans.cluster_centers_])

        loss = 0
        index = 0
        index_array = np.arange(x.shape[0])
        for ite in range(int(maxiter)):
            if ite % update_interval == 0:
                q = self.model.predict(x, verbose=0)
                p = self.target_distribution(q)

                y_pred = q.argmax(1)

                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]
                y_pred_last = np.copy(y_pred)
                if ite > 0 and delta_label < tol:
                    print('delta_label', delta_label, ' < tol ', tol)
                    print("Reached tolerance threshold, Stopping train.")
                    break

            idx = index_array[index * batch_size:min((index+1)*batch_size, x.shape[0])]
            loss = self.model.train_on_batch(x=x[idx], y=p[idx])
            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0

            if ite % save_interval == 0:
                self.model.save_weights(save_dir+"/DEC_model_"+str(ite)+"h5")

            ite += 1

        self.model.save_weights(save_dir+"/DEC_model_final.h5")

        return y_pred


def main():
    data = read_data()
    data.resize(len(data), 256 * 3)
    # temporary
    n_cluster = 5
    update_interval = 13
    pretrain_epochs = 30
    batch_size = 10
    init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution="uniform")
    pretrain_optimizer = Adam()
    dec = DEC(dims=[256 * 3, 500, 500, 2000, 10], n_cluster=n_cluster, init=init)

    dec.pretrain(x=data, optimizer=pretrain_optimizer, epochs=pretrain_epochs, batch_size=batch_size)
    dec.model.summary()
    t0 = time()
    dec.compile(optimizer=Adam(), loss='kld')
    y_pred = dec.fit(data, maxiter=200, batch_size=batch_size, update_interval=update_interval)
    pd.DataFrame(y_pred).to_csv("processed_data/dec_cluster_result.csv", index=False, mode='w', sep=',', header=False)
    print('clustering time', (time() - t0))


if __name__ == '__main__':
    main()

